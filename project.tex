%
% MATH 310 -- Risto Atanasov
% Western Carolina University
%
% Joseph Hunt, Bobby Wertman, Tyler McKinney, 
%
\documentclass{article}
\usepackage{cite}
\usepackage{listings}
\usepackage{color}
\usepackage{appendix}
\usepackage[letterpaper]{geometry}
\usepackage{verbatim}
\title{Sorting Algorithms}
\author{Joseph Randall Hunt\\
Tyler McKinney\\
Bobby Wertman\\
Marco Anton\\
Western Carolina University,\\
Cullowhee, North Carolina\\
}
\date{\today}

\begin{document}
\maketitle
\section{Project Proposal}
%TODO
   \subsection{Goals}
    The goal for this project is to create a simple and straightforward
implementation of several sorts and proceed to analyze those sorts to better
understand BigOh, sorting, and algorithmic complexity. The main goal of this
project is to learn more about BigOh's formal definition and how an
algorithm's efficiency is determined. Ancillary goals
   \subsection{Plan}
    More specifically, we will proceed by implementing Merge Sort, Quick Sort,
    and Shell Sort and then analyzing the efficiency of each sort. A worst-case, 
    best-case, and average-case analysis would be optimal but it might not be
    time-efficient.
\section{Sorts}
   \subsection{Merge Sort}
      \subsubsection{Algorithm}
        Merge sort is a comparison-based sorting algorithm, based on the
divide-and-conquer design. Its average and worst cases are both $n log(n)$,
and its best case is $\Omega(n)$. Invented in 1945 by John von Neumann, it
exploits the fact that combining two lists of sorted data is a linear-time
process. \cite{introalg} \subsubsection{Efficiency} Merge sort's complexity is
$n log(n)$ for all cases, but the number of computations performed changes
between best and worst cases. In the all cases, Merge Sort performs $log(n)$
splits of $n$ elements, putting the efficiency of this phase at $n log(n)$.
However, when merging, the number of operations varies based on the data.
        \paragraph{Worst Case}
          In the worst case, the input data is interleaved in such a way that
          for each step of the merging process, both lists of elements are
          traversed in full before the merge is complete.  This results in $n$
          comparisons over $log(n)$ levels (complexity $n log(n)$, and brings
          the total number of operations for the sort to $2n log(n)$, so the
          efficiency class is $O(n log(n))$.
          $$\begin{array}{rrl} Base\ case: & T(1) & = 0 \\

          Recurrence: & T(n) & = 2 T(\frac{n}{2}) + n \\

          Divide\ both\ sides\ by\ n: & \frac{T(n)}{n} & = \frac{2
              T(\frac{n}{2}) + n}{n} \\

          Simplify: & \frac{T(n)}{n} & = \frac{T(\frac{n}{2})}{\frac{n}{2}} + 1 \\

          Telescope: & & = \frac{T(\frac{n}{4})}{\frac{n}{4}} + 1 + 1 \\

          & \vdots & \\

          & & = \frac{T(\frac{n}{n})}{\frac{n}{n}} + 1 + 1 + \dots + 1 \\
          
          & \frac{T(n)}{n} & = log_2 n \\

          Solve: & T(n) & = n log_2 n \\
          \end{array}$$

        \paragraph{Best Case}
          In the best case, the input data is already in order, allowing the
          merge operations to traverse only the first list, ignoring the second.
          This cuts the number of operations in half compared to the worst
          case, so it performs $\frac{n}{2}$ comparisons on $log(n)$ levels,
          which is $\frac{n log(n)}{2}$ operations for the merge phase and
          $\frac{3n log(n)}{2}$ for the sort, giving us the complexity of
          $\Omega(n log(n))$.  The proof of the efficiency is the same as
          above, replacing the $ + n$ in the recurrence relation with $ +
          \frac{n}{2}$.
        \paragraph{Code}
          The actual code for the algorithm is available in \textbf{Listing
          \ref{code:mergesort}}.  This particular implementation uses an
          optimization that switches to insertion sort on small arrays.  This
          speeds up the algorithm because it allows the small data set to fit
          entirely in cache along with the small amount of code associated with
          the insertion sort algorithm.  See \textbf{Listing
          \ref{code:mergesort-parallel}} for a parallel implementation of merge
          sort.
      \subsubsection{Applications}
        Merge sort is useful in applications where the data set will not fit
        entirely into memory.  This allows for the data to be read in from disk
        and sorted as it is read, thus requiring a very small memory footprint.
        In addition, when time complexity needs to be guaranteed, merge sort is
        preferable over quick sort, as quick sort's worst case is $O(n^2)$.
   \subsection{Quick Sort}
      \subsubsection{Algorithm}Quick sort is a fast algorithm, developed
by Tony Hoare, that has a 
best and average running time of $O(nlog(n))$ and a worst-case  of
$O(n^2)$.\\
Quick sort first divides a large list into two smaller lists with respect
to a ``pivot" (a pivot is a random item selected from the list). The 
first list contains items that are smaller and the second list contains 
items that are larger than the pivot. Finally, apply quick sort to the
smaller and larger lists and return the ordered list. 
According to Weiss\cite{weiss}  the basic algorithm to sort an array $S$ 
consist of the following four steps:
\begin{enumerate}
\item If the number of elements in $S$ is $0$ or $1$, then return.
\item Pick any element $v$ in $S$. This is called the \textbf{pivot}.
\item \textbf{Partition} $S - \{v\}$ (the remaining elements in $S$)
into two disjoint groups: $S_1 = \{x \in S - \{v\}|x\leq v\}$, and $S_2
= \{x \in S - \{v\}|x \geq v\}$.
\item Return $\{$quick sort$(S_1)$ followed by $v$ followed by
quick sort$(S_2)\}$
\end{enumerate}
\subsubsection{Efficiency}
Like merge sort, quick sort is recursive; therefore, its analysis requires
solving a recurrence formula.\\
\begin{itemize}
\item\emph{Worst Case Analysis}\\
To set the recurrence relation on this case, one assumes that the pivot
that is chosen is always the smallest in the list. Therefore, there is 
only going to be one list (the list containing the largest elements).
The recurrence relation on the worst case is:
$$T(1) = 1$$
$$T(n) = T(n-1) + Cn\mbox{, } n > 1$$
Where $T(n-1)$ are the recursions and $c*n$ is the linear time spent in
the partition (selecting a pivot); moreover, in the base case, we just
sort $0$ or $1$ item, we can do this in constant time. To solve this
recursion, 
the method of Backward substitution is used:\\
$$\begin{array}{lclcl}
 &  & &  & substitute: T(n - 1) = T(n - 2) + C(n - 1)\\
T(n - 2) + C(n - 1) + Cn & & &;& substitute: T(n - 2) = T(n - 3) + C(n -
2)\\
T(n - 3)) + C(n - 2) + C(n - 1) + Cn & & &;& substitute: T(n - 3) = T(n
- 4) \\
\vdots & & & & \\
\end{array}$$
$=> T(n - (n - 1)) = C(n - n) + C(n-(n - 1)) + ... + Cn$\\
$= \displaystyle\sum_{i = 2}^{n}Ci = C\left(\frac{n(n + 1)}{2}\right)$
Therefore, in this case quick sort is of $O(n^2)$\\

\item\emph{Best Case Analysis}\\
For the best case analysis we have to choose the pivot such that it is
always in the middle. Therefore our new recurrence relation is:
$$T(n) = 2T(n/2) + Cn$$
$$T(1) = 1$$
Note that the recursion went from $T(n - 1)$ to $T(n/2)$. This is
because
of the selection of the pivot; the pivot has been selected such that the
list is divided into two equal and smaller lists. These smaller lists
are
performed recursively (this is the reason for the $2$ in front of the
recurrence relation) until $T(1)$ where we only return the element.\\
This recurrence relation could be solved using the ``Master
Theorem"\cite{levitin}.\\
$$a = 2; b = 2; d = 1$$
$$\mbox{CASE II: } a = b^d  =>  2 = 2^1$$
Therefore the efficiency class in this case is: $\Theta(n^dlogn) =
\Theta(nlogn)$\\

%\item\emph{Average Case Analysis}\\
\end{itemize}
As analyzed before, the pivot can be chosen in many ways; however, this 
might make the quick sort algorithm to perform poorly. As stated in the 
\emph{Best Case Analysis}, in order for quick sort to perform better, 
one has to select a pivot to be in the center of the array, but this 
might be an issue when we have a list of odd elements. 
An efficient way of choosing the pivot is choosing the element of 
intermediate value between three elements in the list, more
specifically, 
the three elements chosen from
the list have to be the first, last and middle position (middle
element would be $\lceil N/2 \rceil$ where $N =$ number of elements in a 
list). After selecting them, they need to be sorted and the pivot
selected 
would be the middle element. This method is called \emph{Median-of-Three
partitioning}. Not only this method will help choose a good pivot, but
also, will help reduce the number of elements to check. The elements in
the
first and last position are in the correct side of their corresponding
lists; therefore, they don't have to be checked when performing
quicksort.\\
\begin{itemize}
\item QuickSort Example.\\
The following list is Sorted using the quick sort algorithm:\\
$$S = 3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5$$

\begin{itemize}
\item Find "Pivot''\\
Left index = $0$; right index = $10$; center = $5$\\
Sort left, right and center (compare the elements of these indexes and
put the largest on the right index, and the smallest on the left
index).\\
The pivot is the element in position ``center". For the next step, one
needs to put the pivot on position $(n - 1)$ of the array ($n$ is the
size of the array). This yields the following array $3 1 4 1 5 3 2 6
5 4 5 (5) 9$. The item between parentheses is the pivot.\\
\item Partition array into two groups:
Set i to $1$ and j to $n - 2$ (note that left and  right items are on
the
correct side of the array).\\
Transverse through the array until i is greater or equal to the pivot
and j
is smaller or equal to the pivot; in this case until $i = 4$ and
$j = 8$, and swap the items in i and j. This produces the following
array:
$3 1 4 1 5 4 2 6 5 (5) 9$\\
On the next iteration i and j have crossed. No swap is performed;
however, we need to restore the pivot to its correct position (i).\\
Pivot in correct position: $3 1 4 1 5 3 2 (5) 5 6 9$\\
After this step, it is known that the pivot is sorted (smaller elements
are on left of pivot and bigger elements are on right of pivot).\\
\item Quicksort the remaining two groups \\
We now proceed to sort elements to the left of the pivot (smaller
elements) and elements to the right of the pivot (bigger
elements).\\
\end{itemize}

\centering\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$3$ & $1$ & $4$ & $1$ & $5$ & $3$ & $2$ & $(5)$ & $5$ & $6$ & $9$\\
\hline
$3$ & $1$ & $4$ & $(1)$ & $5$ & $3$ & $2$ & $|5|$ & $5$ & $6$ &
$9$\\ \hline
$1$ & $1$ & $4$ & $(3)$ & $5$ & $3$ & $2$ & $|5|$ & $5$ & $6$ &
$9$\\ \hline
$1$ & $1$ & $4$ & $(2)$ & $5$ & $3$ & $3$ & $|5|$ & $5$ & $6$ &
$9$\\ \hline
$1$ & $1$ & $4$ & $3$ & $5$ & $(2)$ & $3$ & $|5|$ & $5$ & $6$ &
$9$\\ \hline
$1$ & $1$ & $|2|$ & $3$ & $5$ & $4$ & $3$ & $|5|$ & $5$ & $6$ &
$9$\\ \hline
$|1|$ & $|1|$ & $|2|$ & $3$ & $5$ & $4$ & $3$ & $|5|$ & $5$ & $6$ &
$9$\\ \hline
$|1|$ & $|1|$ & $|2|$ & $3$ & $(5)$ & $4$ & $3$ & $|5|$ & $5$ & $6$
& $9$\\ \hline
$|1|$ & $|1|$ & $|2|$ & $3$ & $(3)$ & $4$ & $5$ & $|5|$ & $5$ & $6$
& $9$\\ \hline
$|1|$ & $|1|$ & $|2|$ & $3$ & $4$ & $(3)$ & $5$ & $|5|$ & $5$ & $6$
& $9$\\ \hline
$|1|$ & $|1|$ & $|2|$ & $|3|$ & $|3|$ & $|4|$ & $|5|$ & $|5|$ &
$|5|$ &
$|6|$ & $|9|$\\ \hline
\end{tabular}
\end{itemize}
\subsubsection{Applications}
   \subsection{Shell Sort}
   \subsubsection{Algorithm}
        The Shell Sort was developed by Donald Shell in 1959. Formally called
        Shell's Sort, the algorithm was the first sorting method to break the 
        quadratic time, however this was not proven until years later. The 
        Shell Sort is made faster my comparing elements at a distance rather 
        than only comparing elements side by side. This sorting method has
        several pros and cons\cite{introalg}. Mainly, Shell Sort is the
        fastest $N^2$ based sort,however, its computational complexity can be
        difficult to calculate\cite{princton}. Shell Sort is reasonably easy to
        implement and also easy to comprehend as seen in \textbf{Listing
        \ref{code:shellsort}}. Donald Shell's sorting algorithm can be
        relativly easy to visualize.
                \begin{enumerate}
            \item Take the first element, $h_0$.
            \item Compare it to the $h_{k_{th}}$ element. For example, if the 
            gap is 5, compare to the $5^{th}$ element. 
            \item If the $h_{k_{th}}$ is smaller, switch the elements.
            \item Advance the first element by the gap and compare to the next 
            element one gap ahead.
            \item Repeat the above, and, if the element one gap BELOW the first
            element is more, switch the elements and repeat.
        \end{enumerate}
        \begin{center}
            \begin{tabular}{ | c | c | c | c | c | c | c | c | c | c | }
                \hline 
                \multicolumn{10}{|c|}{Initial Set} \\
                \hline
                $3$ & $8$ & $6$ & $2$ & $-5$ & $9$ & $1$ & $4$ & $7$ & $5$ \\ 
                \hline
                \multicolumn{10}{|c|}
                            {$gap = \lfloor set.length\rfloor / 2 = 5$} \\
                \hline
                $|-5|$ & $8$ & $6$ & $2$ & $3$ & $9$ & $1$ & $4$ & $7$ & $5$ \\
                \hline
                $|-5|$ & $8$ & $6$ & $2$ & $3$ & $9$ & $1$ & $4$ & $7$ & $5$ \\
                \hline
                $|-5|$ & $8$ & $1$ & $2$ & $3$ & $9$ & $6$ & $4$ & $7$ & $5$ \\
                \hline
                $|-5|$ & $8$ & $1$ & $2$ & $3$ & $9$ & $6$ & $4$ & $7$ & $5$ \\
                \hline
                $|-5|$ & $8$ & $1$ & $2$ & $3$ & $9$ & $6$ & $4$ & $7$ & $5$ \\
                \hline
                $|-5|$ & $8$ & $1$ & $2$ & $3$ & $5$ & $6$ & $4$ & $7$ & $9$ \\
                \hline
                \multicolumn{10}{|c|}{$gap = \lfloor gap / 2\rfloor = 2$} \\
                \hline
                $|-5|$ & $8$ & $1$ & $2$ & $3$ & $5$ & $6$ & $4$ & $7$ & $9$ \\
                \hline
                $|-5|$ & $2$ & $1$ & $8$ & $3$ & $5$ & $6$ & $4$ & $7$ & $9$ \\
                \hline
                $|-5|$ & $2$ & $1$ & $8$ & $3$ & $5$ & $6$ & $4$ & $7$ & $9$ \\
                \hline
                $|-5|$ & $2$ & $1$ & $5$ & $3$ & $8$ & $6$ & $4$ & $7$ & $9$ \\
                \hline
                $|-5|$ & $2$ & $1$ & $5$ & $3$ & $8$ & $6$ & $4$ & $7$ & $9$ \\
                \hline
                $|-5|$ & $2$ & $1$ & $5$ & $3$ & $4$ & $6$ & $8$ & $7$ & $9$ \\
                \hline
                $|-5|$ & $2$ & $1$ & $4$ & $3$ & $5$ & $6$ & $8$ & $7$ & $9$ \\
                \hline
                $|-5|$ & $2$ & $1$ & $4$ & $3$ & $5$ & $6$ & $8$ & $7$ & $9$ \\
                \hline
                \multicolumn{10}{|c|}{$gap = \lfloor gap / 2\rfloor = 1$} \\
                \multicolumn{10}{|c|}{Insertion Sort} \\
                \hline
                $|-5|$ & $2$ & $1$ & $4$ & $3$ & $5$ & $6$ & $8$ & $7$ & $9$ \\
                \hline
                $|-5|$ & $|1|$ & $2$ & $4$ & $3$ & $5$ & $6$ & $8$ & $7$ & $9$
                \\
                \hline
                $|-5|$ & $|1|$ & $|2|$ & $4$ & $3$ & $5$ & $6$ & $8$ & $7$ &
                $9$ \\
                \hline
                $|-5|$ & $|1|$ & $|2|$ & $|3|$ & $4$ & $5$ & $6$ & $8$ & $7$ &
                $9$ \\
                \hline
                $|-5|$ & $|1|$ & $|2|$ & $|3|$ & $|4|$ & $5$ & $6$ & $8$ & $7$
                & $9$ \\
                \hline
                $|-5|$ & $|1|$ & $|2|$ & $|3|$ & $|4|$ & $|5|$ & $6$ & $8$ &
                $7$ & $9$ \\
                \hline
                $|-5|$ & $|1|$ & $|2|$ & $|3|$ & $|4|$ & $|5|$ & $|6|$ & $8$ &
                $7$ & $9$ \\
                \hline
                $|-5|$ & $|1|$ & $|2|$ & $|3|$ & $|4|$ & $|5|$ & $|6|$ & $|7|$
                & $8$ & $9$ \\
                \hline
                $|-5|$ & $|1|$ & $|2|$ & $|3|$ & $|4|$ & $|5|$ & $|6|$ & $|7|$
                & $|8|$ & $9$ \\
                \hline
                $|-5|$ & $|1|$ & $|2|$ & $|3|$ & $|4|$ & $|5|$ & $|6|$ & $|7|$
                & $|8|$ & $|9|$ \\
                \hline

                          \end{tabular}
        \end{center}
    \subsubsection{Efficiency}
        The Shell Sort can sort an array of elements faster than the insertion
        sort by comparing elements across a gap as opposed to only comparing
        adjacent elements. This gap is reduced and the set is compared
        repeatedly until the gap equals 1. At this point the set can be sorted
        with a simple, and fast, insertion sort. It is because of this that the
        Shell Sort is known as the \textquotedblleft diminishing increment sort
        \textquotedblright \cite{education}. This gap is also the reason that
        the computational complexity is difficult to find, it varies based on
        the gap. Several mathematicians have attempted to find the best
        \textquotedblleft gap sequence \textquotedblright. With the discovery
        of the algorithm, Donald Shell used to floor function of $gap / 2$.
        This is also the gap sequence used in the implementation for this
        project. The worst case complexity for this gap sequence is
        $\Theta (N/2^k)$. Vaughan Pratt, who developed \textquotedblleft
        Pratt's theorem \textquotedblright for tree spanning developed his own
        gap sequence beginning with $h= \{ 1, 3, 7, 15, 31, 63, 127, 255, 511
        \}$. Pratt's gap sequence has a worst case time of $\Theta (N^{3/2})$.
        Several others have their own gap sequence and experts in the
        mathematical field continue to look for the fastest gap
        sequence\cite{princton}. The best case for Shell Sort is $N^2$ when the
        set is already sorted and the gap is 1, the sort is a completed 
        insertion sort.\\
    \subsubsection{Applications}
        Shell's sort is not seen often in todays time. Some implemetations 
        exists within the C standard library including the \textquotedblleft 
        uClibc\textquotedblright which supports Linux in embedded systems
        \cite{uClibc}. The Shell Sort also exists with the Linux kernel itself
        \cite{kernel}. Shell's Sort is not used do to is slower speed and 
        greater number of computations over other sorting algorithms such as 
        Quick Sort.\\
\section{Conclusions}
Through this project we have learned quite a lot about efficiency analysis and
sorting methodologies. It is interesting to note that while Quick sort has the
worst worst-case efficiency it is empirically the fastest of the listed sorts.
This leads one to believe that BigOh notation is not the only concern when
attempting to design an algorithm. Another thoughtful note gleaned from this
is that not all algorithms immediately lend themselves to a simple analysis.
Shell sort for instance is hard to express purely in mathematics and therefor
hard to analyze.

\begin{appendices}
\section{Code Examples}
\lstset{
    language=Go,
    basicstyle=\footnotesize,
    keywordstyle=\bfseries\color[rgb]{0.8,0.6,0.1},
    commentstyle=\scriptsize\color[rgb]{0.133,0.133,0.545},
    stringstyle=\ttfamily\color[rgb]{0.627,0.126,0.941},
    numbers=left,
    frame=single,
    stepnumber=1,
    identifierstyle=\ttfamily,
    tabsize=2
}
\lstinputlisting[caption={Parallel implementation of Merge Sort in Go},label=code:mergesort-parallel]{src/parallel.go}
\lstinputlisting[caption={Go implementation of MergeSort},label=code:mergesort]{src/merge.go}

\lstset{
    language=Java
}
\lstinputlisting[caption={Java implementation of Shell Sort in Java},label=code:shellsort]{src/ShellSort.java}
\lstinputlisting[caption={Java implementation of Quick Sort in Java},label=code:quicksort]{src/QuickSort.java}
\end{appendices}

\bibliography{sources}
\bibliographystyle{plain}
\end{document}
